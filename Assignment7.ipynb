{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8edSiHPi2N5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dropout,AveragePooling2D, merge, Activation,SeparableConv2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Reshape, Activation, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hkvQwOH2cCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNnGm8Tv2fR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "47884af9-42df-4efa-c808-ca9e923dffbf"
      },
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "# Layer 1\n",
        "x = SeparableConv2D(32, (5,5),border_mode='same', name='Sep_conv_1')(input)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_1')(x)\n",
        "\n",
        "layer1 = x\n",
        "\n",
        "# Layer 2\n",
        "x = Conv2D(64, (5,5), name='Conv_1',border_mode='same')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_2')(x)\n",
        "\n",
        "layer2 = x\n",
        "\n",
        "# Layer 3\n",
        "x = Conv2D(128, (5,5), name='Conv_2', border_mode='same')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_3')(x)\n",
        "\n",
        "layer3 = x\n",
        "\n",
        "skip1 = concatenate([layer1, layer3])\n",
        "\n",
        "# Layer 4\n",
        "x = SeparableConv2D(256, (5,5), name='Sep_conv_2',border_mode='same')(skip1)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_4')(x)\n",
        "\n",
        "layer4 = x\n",
        "\n",
        "# Layer 5\n",
        "skip2 = concatenate([layer1, layer4])\n",
        "\n",
        "x = MaxPooling2D(pool_size=(2, 2))(skip2)\n",
        "\n",
        "\n",
        "# Layer 6\n",
        "x = SeparableConv2D(32, (3,3), border_mode='same', name='Sep_conv_3', use_bias=False)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_6')(x)\n",
        "\n",
        "layer6 = x\n",
        "\n",
        "# Layer 7\n",
        "skip3 = concatenate([layer1, layer4])\n",
        "\n",
        "#layer61 = keras.layers.UpSampling3D(size=(2, 2, 1), data_format=None)(layer6)\n",
        "skip31 = AveragePooling2D(2)(skip3)\n",
        "\n",
        "x = concatenate([skip31, layer6])\n",
        "x = Conv2D(64, (5,5), name='Conv_3', border_mode='same')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_7')(x)\n",
        "\n",
        "layer7 = x\n",
        "print(layer7.shape)\n",
        "\n",
        "# Layer 8\n",
        "skip4 = concatenate([layer3, layer4])\n",
        "skip41 = AveragePooling2D(2)(skip4)\n",
        "\n",
        "skip5 = concatenate([layer6, layer7])\n",
        "\n",
        "x = concatenate([skip41,skip5])\n",
        "\n",
        "x = SeparableConv2D(128, (3,3), border_mode='same', name='Sep_conv_4', use_bias=False)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_8')(x)\n",
        "\n",
        "layer8 = x\n",
        "\n",
        "# Layer 9\n",
        "skip6 = concatenate([layer1,layer3, layer4])\n",
        "skip61 = AveragePooling2D(2)(skip6)\n",
        "\n",
        "skip7 = concatenate([layer6,layer7, layer8])\n",
        "\n",
        "x = concatenate([skip61,skip7])\n",
        "\n",
        "x = SeparableConv2D(256, (5,5), border_mode='same', name='Sep_conv_5', use_bias=False)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_9')(x)\n",
        "\n",
        "layer9 = x\n",
        "\n",
        "# Layer 10\n",
        "skip8 = concatenate([layer1, layer4])\n",
        "skip81 = AveragePooling2D(2)(skip8)\n",
        "\n",
        "skip9 = concatenate([layer6,layer8, layer9])\n",
        "\n",
        "x = concatenate([skip81,skip9])\n",
        "\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 11\n",
        "skip10 = AveragePooling2D(2)(layer7)\n",
        "x = concatenate([skip10,x])\n",
        "x = Conv2D(32, (5,5), name='Conv_4',border_mode='same')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_11')(x)\n",
        "\n",
        "layer11 = x\n",
        "\n",
        "# Layer 12\n",
        "skip11 = concatenate([layer1, layer2])\n",
        "skip111 = AveragePooling2D(4)(skip11)\n",
        "\n",
        "skip12 = AveragePooling2D(2)(layer8)\n",
        "\n",
        "x = concatenate([skip111,skip12,x])\n",
        "\n",
        "x = SeparableConv2D(64, (5,5), border_mode='same', name='Sep_conv_6', use_bias=False)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_12')(x)\n",
        "\n",
        "layer12 = x\n",
        "\n",
        "# Layer 13\n",
        "skip13 = concatenate([layer2, layer3])\n",
        "skip131 = AveragePooling2D(4)(skip13)\n",
        "\n",
        "skip14 = AveragePooling2D(2)(layer6)\n",
        "\n",
        "x = concatenate([layer12,layer11,skip131,skip14])\n",
        "\n",
        "x = Conv2D(128, (3,3), name='Conv_5',border_mode='same',)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_13')(x)\n",
        "\n",
        "layer13 = x\n",
        "\n",
        "# Layer 14\n",
        "\n",
        "skip15 = concatenate([layer12, layer13])\n",
        "\n",
        "skip16 = concatenate([layer6, layer8])\n",
        "skip161 = AveragePooling2D(2)(skip16)\n",
        "\n",
        "skip17 = concatenate([layer1, layer3,layer4])\n",
        "skip171 = AveragePooling2D(4)(skip17)\n",
        "\n",
        "x = concatenate([skip15,skip161,skip171])\n",
        "\n",
        "x = SeparableConv2D(256, (5,5), border_mode='same', name='Sep_conv_7', use_bias=False)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = BatchNormalization(name='norm_14')(x)\n",
        "\n",
        "layer14 = x\n",
        "\n",
        "# Layer 15\n",
        "\n",
        "skip18 = concatenate([layer14, layer12])\n",
        "skip19 = AveragePooling2D(4)(layer4)\n",
        "skip20 = AveragePooling2D(2)(layer8)\n",
        "\n",
        "x = concatenate([skip18,skip19,skip20])\n",
        "\n",
        "x = Conv2D(256, 3, 3,)(x)\n",
        "x = Conv2D(256, 3, 3,)(x)\n",
        "x = Conv2D(256, 3, 3,)(x)\n",
        "x = Conv2D(10, 2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "output = Activation('softmax')(x)\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (5, 5), name=\"Sep_conv_1\", padding=\"same\")`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), name=\"Conv_1\", padding=\"same\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), name=\"Conv_2\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(256, (5, 5), name=\"Sep_conv_2\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (3, 3), name=\"Sep_conv_3\", use_bias=False, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), name=\"Conv_3\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(128, (3, 3), name=\"Sep_conv_4\", use_bias=False, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(256, (5, 5), name=\"Sep_conv_5\", use_bias=False, padding=\"same\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 16, 16, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), name=\"Conv_4\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:115: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(64, (5, 5), name=\"Sep_conv_6\", use_bias=False, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:129: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), name=\"Conv_5\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:147: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(256, (5, 5), name=\"Sep_conv_7\", use_bias=False, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:161: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:162: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:163: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jeh0VAxy26NV",
        "colab_type": "code",
        "outputId": "b2800597-9238-4dd3-f372-a7eb7f8a8ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3485
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_33 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Sep_conv_1 (SeparableConv2D)    (None, 32, 32, 32)   203         input_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 32, 32, 32)   0           Sep_conv_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "norm_1 (BatchNormalization)     (None, 32, 32, 32)   128         activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1 (Conv2D)                 (None, 32, 32, 64)   51264       norm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 32, 32, 64)   0           Conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_2 (BatchNormalization)     (None, 32, 32, 64)   256         activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Conv_2 (Conv2D)                 (None, 32, 32, 128)  204928      norm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 32, 32, 128)  0           Conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_3 (BatchNormalization)     (None, 32, 32, 128)  512         activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_280 (Concatenate)   (None, 32, 32, 160)  0           norm_1[0][0]                     \n",
            "                                                                 norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Sep_conv_2 (SeparableConv2D)    (None, 32, 32, 256)  45216       concatenate_280[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 32, 32, 256)  0           Sep_conv_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "norm_4 (BatchNormalization)     (None, 32, 32, 256)  1024        activation_270[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_281 (Concatenate)   (None, 32, 32, 288)  0           norm_1[0][0]                     \n",
            "                                                                 norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling2D) (None, 16, 16, 288)  0           concatenate_281[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Sep_conv_3 (SeparableConv2D)    (None, 16, 16, 32)   11808       max_pooling2d_47[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 16, 16, 32)   0           Sep_conv_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_282 (Concatenate)   (None, 32, 32, 288)  0           norm_1[0][0]                     \n",
            "                                                                 norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_6 (BatchNormalization)     (None, 16, 16, 32)   128         activation_271[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_98 (AveragePo (None, 16, 16, 288)  0           concatenate_282[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_283 (Concatenate)   (None, 16, 16, 320)  0           average_pooling2d_98[0][0]       \n",
            "                                                                 norm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Conv_3 (Conv2D)                 (None, 16, 16, 64)   512064      concatenate_283[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 16, 16, 64)   0           Conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_284 (Concatenate)   (None, 32, 32, 384)  0           norm_3[0][0]                     \n",
            "                                                                 norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_7 (BatchNormalization)     (None, 16, 16, 64)   256         activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_99 (AveragePo (None, 16, 16, 384)  0           concatenate_284[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_285 (Concatenate)   (None, 16, 16, 96)   0           norm_6[0][0]                     \n",
            "                                                                 norm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_286 (Concatenate)   (None, 16, 16, 480)  0           average_pooling2d_99[0][0]       \n",
            "                                                                 concatenate_285[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Sep_conv_4 (SeparableConv2D)    (None, 16, 16, 128)  65760       concatenate_286[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 16, 16, 128)  0           Sep_conv_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "norm_8 (BatchNormalization)     (None, 16, 16, 128)  512         activation_273[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_287 (Concatenate)   (None, 32, 32, 416)  0           norm_1[0][0]                     \n",
            "                                                                 norm_3[0][0]                     \n",
            "                                                                 norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_100 (AverageP (None, 16, 16, 416)  0           concatenate_287[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_288 (Concatenate)   (None, 16, 16, 224)  0           norm_6[0][0]                     \n",
            "                                                                 norm_7[0][0]                     \n",
            "                                                                 norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_289 (Concatenate)   (None, 16, 16, 640)  0           average_pooling2d_100[0][0]      \n",
            "                                                                 concatenate_288[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Sep_conv_5 (SeparableConv2D)    (None, 16, 16, 256)  179840      concatenate_289[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 16, 16, 256)  0           Sep_conv_5[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_290 (Concatenate)   (None, 32, 32, 288)  0           norm_1[0][0]                     \n",
            "                                                                 norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_9 (BatchNormalization)     (None, 16, 16, 256)  1024        activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_101 (AverageP (None, 16, 16, 288)  0           concatenate_290[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_291 (Concatenate)   (None, 16, 16, 416)  0           norm_6[0][0]                     \n",
            "                                                                 norm_8[0][0]                     \n",
            "                                                                 norm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_292 (Concatenate)   (None, 16, 16, 704)  0           average_pooling2d_101[0][0]      \n",
            "                                                                 concatenate_291[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_102 (AverageP (None, 8, 8, 64)     0           norm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling2D) (None, 8, 8, 704)    0           concatenate_292[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_293 (Concatenate)   (None, 8, 8, 768)    0           average_pooling2d_102[0][0]      \n",
            "                                                                 max_pooling2d_48[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "Conv_4 (Conv2D)                 (None, 8, 8, 32)     614432      concatenate_293[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_294 (Concatenate)   (None, 32, 32, 96)   0           norm_1[0][0]                     \n",
            "                                                                 norm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 8, 8, 32)     0           Conv_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_103 (AverageP (None, 8, 8, 96)     0           concatenate_294[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_104 (AverageP (None, 8, 8, 128)    0           norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_11 (BatchNormalization)    (None, 8, 8, 32)     128         activation_275[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_295 (Concatenate)   (None, 8, 8, 256)    0           average_pooling2d_103[0][0]      \n",
            "                                                                 average_pooling2d_104[0][0]      \n",
            "                                                                 norm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Sep_conv_6 (SeparableConv2D)    (None, 8, 8, 64)     22784       concatenate_295[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 8, 8, 64)     0           Sep_conv_6[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_296 (Concatenate)   (None, 32, 32, 192)  0           norm_2[0][0]                     \n",
            "                                                                 norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_12 (BatchNormalization)    (None, 8, 8, 64)     256         activation_276[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_105 (AverageP (None, 8, 8, 192)    0           concatenate_296[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_106 (AverageP (None, 8, 8, 32)     0           norm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_297 (Concatenate)   (None, 8, 8, 320)    0           norm_12[0][0]                    \n",
            "                                                                 norm_11[0][0]                    \n",
            "                                                                 average_pooling2d_105[0][0]      \n",
            "                                                                 average_pooling2d_106[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Conv_5 (Conv2D)                 (None, 8, 8, 128)    368768      concatenate_297[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 8, 8, 128)    0           Conv_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "norm_13 (BatchNormalization)    (None, 8, 8, 128)    512         activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_299 (Concatenate)   (None, 16, 16, 160)  0           norm_6[0][0]                     \n",
            "                                                                 norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_300 (Concatenate)   (None, 32, 32, 416)  0           norm_1[0][0]                     \n",
            "                                                                 norm_3[0][0]                     \n",
            "                                                                 norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_298 (Concatenate)   (None, 8, 8, 192)    0           norm_12[0][0]                    \n",
            "                                                                 norm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_107 (AverageP (None, 8, 8, 160)    0           concatenate_299[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_108 (AverageP (None, 8, 8, 416)    0           concatenate_300[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_301 (Concatenate)   (None, 8, 8, 768)    0           concatenate_298[0][0]            \n",
            "                                                                 average_pooling2d_107[0][0]      \n",
            "                                                                 average_pooling2d_108[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Sep_conv_7 (SeparableConv2D)    (None, 8, 8, 256)    215808      concatenate_301[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 8, 8, 256)    0           Sep_conv_7[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "norm_14 (BatchNormalization)    (None, 8, 8, 256)    1024        activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_302 (Concatenate)   (None, 8, 8, 320)    0           norm_14[0][0]                    \n",
            "                                                                 norm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_109 (AverageP (None, 8, 8, 256)    0           norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_110 (AverageP (None, 8, 8, 128)    0           norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_303 (Concatenate)   (None, 8, 8, 704)    0           concatenate_302[0][0]            \n",
            "                                                                 average_pooling2d_109[0][0]      \n",
            "                                                                 average_pooling2d_110[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 6, 6, 256)    1622272     concatenate_303[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 256)    590080      conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 2, 2, 256)    590080      conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 1, 1, 10)     10250       conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 10)           0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 10)           0           flatten_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,111,317\n",
            "Trainable params: 5,108,437\n",
            "Non-trainable params: 2,880\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apCwOjvZ4Kts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLaFy2AO4TLl",
        "colab_type": "code",
        "outputId": "26c9be29-2002-48c9-c697-679ca12f5ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3522
        }
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0619 05:59:08.303069 140551772518272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 110s 2ms/step - loss: 1.5691 - acc: 0.5227 - val_loss: 1.1944 - val_acc: 0.5852\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 90s 2ms/step - loss: 0.9713 - acc: 0.6627 - val_loss: 0.9644 - val_acc: 0.6673\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.8063 - acc: 0.7220 - val_loss: 0.9895 - val_acc: 0.6653\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.6883 - acc: 0.7633 - val_loss: 0.9412 - val_acc: 0.7117\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.6127 - acc: 0.7905 - val_loss: 1.1164 - val_acc: 0.6560\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.5397 - acc: 0.8159 - val_loss: 0.9710 - val_acc: 0.7199\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.5005 - acc: 0.8318 - val_loss: 1.1705 - val_acc: 0.6753\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4580 - acc: 0.8497 - val_loss: 1.1625 - val_acc: 0.7116\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4387 - acc: 0.8589 - val_loss: 1.4480 - val_acc: 0.6856\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4122 - acc: 0.8717 - val_loss: 1.6274 - val_acc: 0.6508\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4185 - acc: 0.8749 - val_loss: 1.3149 - val_acc: 0.7155\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.3792 - acc: 0.8883 - val_loss: 1.6424 - val_acc: 0.7159\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4075 - acc: 0.8882 - val_loss: 1.4288 - val_acc: 0.7018\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.3863 - acc: 0.8959 - val_loss: 1.5824 - val_acc: 0.7228\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.3822 - acc: 0.9022 - val_loss: 2.0012 - val_acc: 0.7206\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.3488 - acc: 0.9102 - val_loss: 1.8893 - val_acc: 0.6984\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.3145 - acc: 0.9218 - val_loss: 2.1791 - val_acc: 0.7047\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4929 - acc: 0.8978 - val_loss: 1.7930 - val_acc: 0.6975\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4033 - acc: 0.9149 - val_loss: 2.1586 - val_acc: 0.7189\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.3124 - acc: 0.9318 - val_loss: 2.1571 - val_acc: 0.7473\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.3808 - acc: 0.9257 - val_loss: 2.6126 - val_acc: 0.7069\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.4880 - acc: 0.9166 - val_loss: 3.8342 - val_acc: 0.6585\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.5897 - acc: 0.9105 - val_loss: 2.9071 - val_acc: 0.7007\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.6873 - acc: 0.9077 - val_loss: 3.3866 - val_acc: 0.7035\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.7608 - acc: 0.9076 - val_loss: 3.0789 - val_acc: 0.7400\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 89s 2ms/step - loss: 0.9945 - acc: 0.8996 - val_loss: 4.3380 - val_acc: 0.6767\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 88s 2ms/step - loss: 1.9515 - acc: 0.8455 - val_loss: 5.8532 - val_acc: 0.6216\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 88s 2ms/step - loss: 10.1967 - acc: 0.3621 - val_loss: 14.4918 - val_acc: 0.1009\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.5795 - acc: 0.0954 - val_loss: 14.3838 - val_acc: 0.1076\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3790 - acc: 0.1079 - val_loss: 14.3580 - val_acc: 0.1092\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3886 - acc: 0.1073 - val_loss: 14.3548 - val_acc: 0.1094\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3780 - acc: 0.1080 - val_loss: 14.3548 - val_acc: 0.1094\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3828 - acc: 0.1077 - val_loss: 14.3586 - val_acc: 0.1091\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.2204 - acc: 0.1177 - val_loss: 14.1178 - val_acc: 0.1241\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1469 - acc: 0.1223 - val_loss: 14.1388 - val_acc: 0.1228\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1443 - acc: 0.1225 - val_loss: 14.1356 - val_acc: 0.1230\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1459 - acc: 0.1224 - val_loss: 14.1372 - val_acc: 0.1229\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1530 - acc: 0.1219 - val_loss: 14.1388 - val_acc: 0.1228\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1372 - acc: 0.1229 - val_loss: 14.1404 - val_acc: 0.1227\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1459 - acc: 0.1224 - val_loss: 14.1469 - val_acc: 0.1223\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1381 - acc: 0.1228 - val_loss: 14.1420 - val_acc: 0.1226\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1249 - acc: 0.1237 - val_loss: 14.1452 - val_acc: 0.1224\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1449 - acc: 0.1224 - val_loss: 14.1404 - val_acc: 0.1227\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1530 - acc: 0.1219 - val_loss: 14.1356 - val_acc: 0.1230\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1327 - acc: 0.1232 - val_loss: 14.1323 - val_acc: 0.1232\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1375 - acc: 0.1229 - val_loss: 14.1452 - val_acc: 0.1224\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1423 - acc: 0.1226 - val_loss: 14.1469 - val_acc: 0.1223\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1294 - acc: 0.1234 - val_loss: 14.1420 - val_acc: 0.1226\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1398 - acc: 0.1227 - val_loss: 14.1436 - val_acc: 0.1225\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1404 - acc: 0.1227 - val_loss: 14.1420 - val_acc: 0.1226\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1420 - acc: 0.1226 - val_loss: 14.1419 - val_acc: 0.1226\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1391 - acc: 0.1228 - val_loss: 14.1420 - val_acc: 0.1226\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1465 - acc: 0.1223 - val_loss: 14.1420 - val_acc: 0.1226\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1517 - acc: 0.1220 - val_loss: 14.1372 - val_acc: 0.1229\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1414 - acc: 0.1226 - val_loss: 14.1436 - val_acc: 0.1225\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1452 - acc: 0.1224 - val_loss: 14.1420 - val_acc: 0.1226\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.1433 - acc: 0.1225 - val_loss: 14.1378 - val_acc: 0.1228\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.2803 - acc: 0.1140 - val_loss: 14.5417 - val_acc: 0.0978\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4476 - acc: 0.1036 - val_loss: 14.4354 - val_acc: 0.1044\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4466 - acc: 0.1037 - val_loss: 14.4483 - val_acc: 0.1036\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4447 - acc: 0.1038 - val_loss: 14.4434 - val_acc: 0.1039\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4425 - acc: 0.1040 - val_loss: 14.4483 - val_acc: 0.1036\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4454 - acc: 0.1038 - val_loss: 14.4483 - val_acc: 0.1036\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4360 - acc: 0.1044 - val_loss: 14.4499 - val_acc: 0.1035\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4357 - acc: 0.1044 - val_loss: 14.4466 - val_acc: 0.1037\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4437 - acc: 0.1039 - val_loss: 14.4499 - val_acc: 0.1035\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4396 - acc: 0.1041 - val_loss: 14.4466 - val_acc: 0.1037\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4518 - acc: 0.1034 - val_loss: 14.4499 - val_acc: 0.1035\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4428 - acc: 0.1039 - val_loss: 14.4499 - val_acc: 0.1035\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4428 - acc: 0.1039 - val_loss: 14.4450 - val_acc: 0.1038\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4318 - acc: 0.1046 - val_loss: 14.4483 - val_acc: 0.1036\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4473 - acc: 0.1037 - val_loss: 14.4466 - val_acc: 0.1037\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4454 - acc: 0.1038 - val_loss: 14.4450 - val_acc: 0.1038\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.4219 - acc: 0.1052 - val_loss: 14.4676 - val_acc: 0.1024\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3193 - acc: 0.1116 - val_loss: 14.2436 - val_acc: 0.1163\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3232 - acc: 0.1114 - val_loss: 14.2806 - val_acc: 0.1140\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3312 - acc: 0.1109 - val_loss: 14.2790 - val_acc: 0.1141\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3361 - acc: 0.1106 - val_loss: 14.2806 - val_acc: 0.1140\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3341 - acc: 0.1107 - val_loss: 14.2806 - val_acc: 0.1140\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3164 - acc: 0.1118 - val_loss: 14.2790 - val_acc: 0.1141\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3235 - acc: 0.1113 - val_loss: 14.2774 - val_acc: 0.1142\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3222 - acc: 0.1114 - val_loss: 14.2790 - val_acc: 0.1141\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3171 - acc: 0.1117 - val_loss: 14.2806 - val_acc: 0.1140\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3316 - acc: 0.1108 - val_loss: 14.2758 - val_acc: 0.1143\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3274 - acc: 0.1111 - val_loss: 14.2806 - val_acc: 0.1140\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3135 - acc: 0.1120 - val_loss: 14.2822 - val_acc: 0.1139\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3264 - acc: 0.1112 - val_loss: 14.2774 - val_acc: 0.1142\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3193 - acc: 0.1116 - val_loss: 14.2806 - val_acc: 0.1140\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3174 - acc: 0.1117 - val_loss: 14.2774 - val_acc: 0.1142\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3206 - acc: 0.1115 - val_loss: 14.2774 - val_acc: 0.1142\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3222 - acc: 0.1114 - val_loss: 14.2774 - val_acc: 0.1142\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3274 - acc: 0.1111 - val_loss: 14.2790 - val_acc: 0.1141\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3238 - acc: 0.1113 - val_loss: 14.2806 - val_acc: 0.1140\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3190 - acc: 0.1116 - val_loss: 14.2774 - val_acc: 0.1142\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3206 - acc: 0.1115 - val_loss: 14.2774 - val_acc: 0.1142\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3206 - acc: 0.1115 - val_loss: 14.2758 - val_acc: 0.1143\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3158 - acc: 0.1118 - val_loss: 14.2790 - val_acc: 0.1141\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3274 - acc: 0.1111 - val_loss: 14.2758 - val_acc: 0.1143\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3312 - acc: 0.1109 - val_loss: 14.2758 - val_acc: 0.1143\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 87s 2ms/step - loss: 14.3300 - acc: 0.1109 - val_loss: 14.2839 - val_acc: 0.1138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd4060e00f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTfZIGrf4Uyd",
        "colab_type": "code",
        "outputId": "784bbc5b-3e86-4e13-dc3d-862e6a53d369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"Assignment7.h5\")\n",
        "print(\"Saved the model to disk\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 7s 706us/step\n",
            "Test loss: 14.283856028747559\n",
            "Test accuracy: 0.1138\n",
            "Saved the model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyb9TlVGsZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"Assignment7.h5\")\n",
        "print(\"Saved the model to disk\")\n",
        "from google.colab import files\n",
        "\n",
        "files.download('Assignment7.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9A3pesKbUJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}